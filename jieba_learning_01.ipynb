{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jieba\n",
    "\n",
    "## 主要功能:\n",
    "***\n",
    "### 1. 分词\n",
    "\n",
    "* jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型\n",
    "* jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8\n",
    "* jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用\n",
    "* jieba.lcut 以及 jieba.lcut_for_search 直接返回 list\n",
    "* jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding = UTF-8\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from __future__ import unicode_literals\n",
    "import sys,os\n",
    "from whoosh.index import create_in,open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "from jieba.analyse import ChineseAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode:我/来到/北京/清华/清华大学/华大/大学\n",
      "Default Mode:我/来到/北京/清华大学\n",
      "我, 来到, 北京, 清华大学\n",
      "Search Engin Mode小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut('我来到北京清华大学',cut_all=True)\n",
    "print('Full Mode:' + '/'.join(seg_list)) #全模式\n",
    "\n",
    "seg_list = jieba.cut('我来到北京清华大学',cut_all=False)\n",
    "print('Default Mode:' + '/'.join(seg_list)) # 精确模式\n",
    "\n",
    "seg_list = jieba.cut('我来到北京清华大学')\n",
    "print(', '.join(seg_list)) # 默认为精确模式\n",
    "\n",
    "seg_list = jieba.cut_for_search('小明硕士毕业于中国科学院计算所，后在日本京都大学深造') # 搜索引擎模式\n",
    "print('Search Engin Mode'+', '.join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 添加自定义词典\n",
    "***\n",
    "**载入词典**\n",
    "* 开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率\n",
    "* 用法： <b>jieba.load_userdict(file_name)</b> # file_name 为文件类对象或自定义词典的路径\n",
    "* 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "* 词频省略时使用自动计算的能保证分出该词的词频。\n",
    "* 更改分词器（默认为 jieba.dt）的 tmp_dir 和 cache_file 属性，可分别指定缓存文件所在的文件夹及其文件名，用于受限的文件系统。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福/是/创新办/主任/也/是/云计算/方面/的/专家\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('李小福是创新办主任也是云计算方面的专家')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福/是/创新办/主任/也/是/云计算/方面/的/专家\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word('创新办')\n",
    "jieba.add_word('云计算')\n",
    "print('/'.join(jieba.cut('李小福是创新办主任也是云计算方面的专家')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**调整词典**\n",
    "* 使用 <b>add_word(word, freq=None, tag=None) </b>和 <b>del_word(word) </b>可在程序中动态修改词典。\n",
    "\n",
    "* 使用 <b>suggest_freq(segment, tune=True)</b> 可调节单个词语的词频，使其能（或不能）被分出来。\n",
    "\n",
    "* 注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/post/中/将/出错/。\n",
      "如果/放到/post/中/将/出错/。\n",
      "「/台中/」/正确/应该/不会/被/切开\n",
      "「/台中/」/正确/应该/不会/被/切开\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。',HMM = False)))\n",
    "jieba.suggest_freq(('中','将'), tune=True)\n",
    "print('/'.join(jieba.cut('如果放到post中将出错。',HMM = False)))\n",
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开',HMM = False)))\n",
    "jieba.suggest_freq('台中',tune=True)\n",
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开',HMM = False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 关键词抽取\n",
    "\n",
    "### 基于 TF-IDF 算法的关键词抽取\n",
    "\n",
    "> **import jieba.analyse**\n",
    "\n",
    "** jieba.analyse.extract_tags(sentence, topK = 20, withWeight = False, allowPOS = ())**\n",
    "\n",
    "**参数说明**\n",
    "* sentence 为待提取文本\n",
    "* topK 为返回几个TF/IDF 权重最大的关键词,默认值为20\n",
    "* withWeight 为是否一并返回关键词的权重值,默认为False\n",
    "* allowPOS仅包括制定词性的词, 默认为空,既不筛选\n",
    "\n",
    "---\n",
    "\n",
    "** jieba.analyse.TFIDF(idf_path=None) **\n",
    "\n",
    "* 新建 TFIDF 实例，idf_path 为 IDF 频率文件\n",
    "* IDF 逆向文件词频\n",
    "\n",
    "**1. 关键词提取使用逆向文件频率(IDF)文本语料库可以切换成自定义语料库的路径**\n",
    "\n",
    "> <b >用法</b>: jieba.analyse.set_idf_path(file_name)  #file_name为自定义语料库的路径\n",
    "\n",
    "**2. 关键词提取所使用的停用词(stop words)文本语料库可以切换成自定义语料库的路径**\n",
    "\n",
    "> <b>用法:</b> jieba.analyse.set_stop_words(file_name) # file_name 为自定义语料库的路径\n",
    "\n",
    "**3. 关键词也可以一并返回关键词权重值**\n",
    "\n",
    "---\n",
    "\n",
    "### 基于TextRank算法的关键词抽取\n",
    "\n",
    "<b>jieba.analys.textrank(sentence, topK = 20, withWeight = False, allowPos = (\"ns\",\"n\",\"vn\",\"v\")) </b> 直接使用, 接口相同, 之一默认过滤词性\n",
    "\n",
    "<b>jieba.analyse.TextRank()</b>新建自定义TextRank实例\n",
    "> <b>基本思想:</b>\n",
    " 1. 将待抽取关键词的文本进行分词\n",
    " 2. 以固定窗口大小(默认为5, 通过span属性调整), 词之间的共现关系, 构建图\n",
    " 3. 计算图中节点的pagerank, 注意是**无向带权图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所謂;是否;一般;沒有;矯作;退縮;肌迫;雖然;怯懦;真實\n"
     ]
    }
   ],
   "source": [
    "# example for tags extract\n",
    "import jieba.analyse\n",
    "content = '''我沒有心\n",
    "我沒有真實的自我\n",
    "我只有消瘦的臉孔\n",
    "所謂軟弱\n",
    "所謂的順從一向是我\n",
    "的座右銘\n",
    "\n",
    "而我\n",
    "沒有那海洋的寬闊\n",
    "我只要熱情的撫摸\n",
    "所謂空洞\n",
    "所謂不安全感是我\n",
    "的墓誌銘\n",
    "\n",
    "而你\n",
    "是否和我一般怯懦\n",
    "是否和我一般矯作\n",
    "和我一般囉唆\n",
    "\n",
    "而你\n",
    "是否和我一般退縮\n",
    "是否和我一般肌迫\n",
    "一般地困惑\n",
    "\n",
    "我沒有力\n",
    "我沒有滿腔的熱火\n",
    "我只有滿肚的如果\n",
    "所謂勇氣\n",
    "所謂的認同感是我\n",
    "隨便說說\n",
    "\n",
    "而你\n",
    "是否和我一般怯懦\n",
    "是否和我一般矯作\n",
    "是否對你來說\n",
    "只是一場遊戲\n",
    "雖然沒有把握\n",
    "\n",
    "而你\n",
    "是否和我一般退縮\n",
    "是否和我一般肌迫\n",
    "是否對你來說\n",
    "只是逼不得已\n",
    "雖然沒有藉口'''\n",
    "\n",
    "tags = jieba.analyse.extract_tags(content, topK=10)\n",
    "print(\";\".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一般; 所謂; 是否; 只有; 怯懦; 矯作; 退縮; 肌迫; 只是; 有心\n"
     ]
    }
   ],
   "source": [
    "# IDF 逆向文件频率文本语料库切换成自定义语料库\n",
    "jieba.analyse.set_idf_path('idf.txt.big')\n",
    "tags = jieba.analyse.extract_tags(content, topK=10)\n",
    "print(\"; \".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所謂;只有;怯懦;矯作;退縮;肌迫;有心;真實;自我;消瘦\n"
     ]
    }
   ],
   "source": [
    "# stop words 停用词文本语料库切换成自定义的语料库\n",
    "jieba.analyse.set_stop_words('stop_words.txt')\n",
    "tags = jieba.analyse.extract_tags(content, topK=10)\n",
    "print(\";\".join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag: 所謂\tweight: 1.526141\n",
      "tag: 只有\tweight: 0.508714\n",
      "tag: 怯懦\tweight: 0.508714\n",
      "tag: 矯作\tweight: 0.508714\n",
      "tag: 退縮\tweight: 0.508714\n",
      "tag: 肌迫\tweight: 0.508714\n",
      "tag: 有心\tweight: 0.254357\n",
      "tag: 真實\tweight: 0.254357\n",
      "tag: 自我\tweight: 0.254357\n",
      "tag: 消瘦\tweight: 0.254357\n"
     ]
    }
   ],
   "source": [
    "# 返回关键词以及权重\n",
    "tags = jieba.analyse.extract_tags(content, topK=10, withWeight=True)\n",
    "for tag in tags:\n",
    "    print((\"tag: %s\\tweight: %f\")%(tag[0],tag[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 基于 TextRank 算法的关键词抽取的示例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 词性标注\n",
    "---\n",
    "> 1. jieba.posseg.POSTokenizer(tokenizer = None)\n",
    ">> 新建自定义分词器, tokenizer参数可指定内部使用的 jieba.Tokenizer分词器. jieba.posseg.dt为默认词性标注分词器\n",
    "\n",
    "> 2. 标注句子分词后每个词的词性, 采用和ictclas 兼容的标记法\n",
    "\n",
    "> 3. 用法如下:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?jieba.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\JANNEY~1.ZHA\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.758 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 我\tflag: r\n",
      "word: 爱\tflag: v\n",
      "word: 北京\tflag: ns\n",
      "word: 天安门\tflag: ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut('我爱北京天安门')\n",
    "for w, f in words:\n",
    "    print((\"word: %s\\tflag: %s\")%(w,f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 并行分词\n",
    " ---\n",
    " \n",
    " * <b>原理:</b>将目标文本按行分割后, 把各行文本分配到多个python 进行并行分词, 然后归并结果, 从而获得分词速度的客观提升\n",
    " * 基于python 自带的<b>multiprocessing</b>模块, 目前暂不支持windows...\n",
    " * <b>用法:</b>\n",
    " \n",
    " >> <b>jieba.enable_parallel(4)</b> # 开启并行分词模式, 参数为并行进程数\n",
    " >> <b>jieba.disable_parallel()</b> # 关闭并行分词模式\n",
    " \n",
    " * 实验结果: 在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。\n",
    " \n",
    " \n",
    " **注意** 并行分词仅支持默认分词器 jieba.dt 和jieba.posseg.dt\n",
    " \n",
    " ---\n",
    " \n",
    "### 6. Tokenizer: 返回词语在原文的起止位置\n",
    "---\n",
    "**注意**, 输入参数只接受**unicode**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和 \t start: 0 \t end: 2\n",
      "word 有限 \t start: 2 \t end: 4\n",
      "word 服装 \t start: 4 \t end: 6\n",
      "word 饰品 \t start: 6 \t end: 8\n",
      "word 永和 \t start: 8 \t end: 10\n",
      "word 有限公司 \t start: 10 \t end: 14\n"
     ]
    }
   ],
   "source": [
    "# 默认模式\n",
    "result = jieba.tokenize(u'永和有限服装饰品永和有限公司')\n",
    "for tk in result:\n",
    "    print(\"word %s \\t start: %d \\t end: %d\"%(tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和 \t start: 0 \t end: 2\n",
      "word 服装 \t start: 2 \t end: 4\n",
      "word 饰品 \t start: 4 \t end: 6\n",
      "word 有限 \t start: 6 \t end: 8\n",
      "word 公司 \t start: 8 \t end: 10\n",
      "word 有限公司 \t start: 6 \t end: 10\n"
     ]
    }
   ],
   "source": [
    "# 搜索模式\n",
    "result = jieba.tokenize(u'永和服装饰品有限公司', mode = \"search\")\n",
    "for tk in result:\n",
    "    print(\"word %s \\t start: %d \\t end: %d\"%(tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. ChineseAnalyzer for Whoosh 搜索引擎\n",
    "---\n",
    "**引用: ** from jieba.analyse import ChineseAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import sys,os\n",
    "from whoosh.index import create_in,open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "from jieba.analyse import ChineseAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyzer = ChineseAnalyzer()\n",
    "schema = Schema(title = TEXT(stored = True), path = ID(stored = True), content = TEXT(stored = True, analyzer = analyzer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"tmp\"):\n",
    "    os.mkdir(\"tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix = open_dir(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searcher = ix.searcher()\n",
    "parser = QueryParser(\"content\", schema=ix.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of  水果世博园\n",
      "<Hit {'content': '买水果然后来世博园。', 'path': '/c', 'title': 'document3'}>\n",
      "买<b class=\"match term0\">水果</b>然后来<b class=\"match term1\">世博园</b>\n",
      "==========\n",
      "result of  你\n",
      "<Hit {'content': 'The second one 你 中文测试中文 is even more interesting! 吃水果', 'path': '/b', 'title': 'document2'}>\n",
      "second one <b class=\"match term0\">你</b> 中文测试中文 is even more interesting\n",
      "==========\n",
      "result of  first\n",
      "<Hit {'content': 'This is the first document we’ve added!', 'path': '/a', 'title': 'document1'}>\n",
      "<b class=\"match term0\">first</b> document we’ve added\n",
      "==========\n",
      "result of  中文\n",
      "<Hit {'content': 'The second one 你 中文测试中文 is even more interesting! 吃水果', 'path': '/b', 'title': 'document2'}>\n",
      "second one 你 <b class=\"match term0\">中文</b>测试<b class=\"match term0\">中文</b> is even more interesting\n",
      "==========\n",
      "result of  交换机\n",
      "<Hit {'content': '工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作', 'path': '/c', 'title': 'document4'}>\n",
      "干事每月经过下属科室都要亲口交代24口<b class=\"match term0\">交换机</b>等技术性器件的安装工作\n",
      "==========\n",
      "result of  交换\n",
      "<Hit {'content': '咱俩交换一下吧。', 'path': '/c', 'title': 'document4'}>\n",
      "咱俩<b class=\"match term0\">交换</b>一下吧\n",
      "<Hit {'content': '工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作', 'path': '/c', 'title': 'document4'}>\n",
      "干事每月经过下属科室都要亲口交代24口<b class=\"match term0\">交换</b>机等技术性器件的安装工作\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for keyword in (\"水果世博园\",\"你\",\"first\",\"中文\",\"交换机\",\"交换\"):\n",
    "    print(\"result of \",keyword)\n",
    "    q = parser.parse(keyword)\n",
    "    results = searcher.search(q)\n",
    "    #print(results)\n",
    "    for hit in results:\n",
    "        print(hit)\n",
    "        print(hit.highlights('content'))\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result of  水果世博园\n",
    "\n",
    "买<b class=\"match term0\">水果</b>然后来<b class=\"match term1\">世博园</b>\n",
    "***\n",
    "result of  你\n",
    "\n",
    "second one <b class=\"match term0\">你</b> 中文测试中文 is even more interesting\n",
    "***\n",
    "result of  first\n",
    "\n",
    "<b class=\"match term0\">first</b> document we’ve added\n",
    "***\n",
    "result of  中文\n",
    "\n",
    "second one 你 <b class=\"match term0\">中文</b>测试<b class=\"match term0\">中文</b> is even more interesting\n",
    "***\n",
    "result of  交换机\n",
    "\n",
    "干事每月经过下属科室都要亲口交代24口<b class=\"match term0\">交换机</b>等技术性器件的安装工作\n",
    "***\n",
    "result of  交换\n",
    "\n",
    "咱俩<b class=\"match term0\">交换</b>一下吧\n",
    "\n",
    "干事每月经过下属科室都要亲口交代24口<b class=\"match term0\">交换</b>机等技术性器件的安装工作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\n",
      "朋友\n",
      "好朋友\n",
      "是\n",
      "李明\n",
      "我\n",
      "爱\n",
      "北京\n",
      "天安\n",
      "天安门\n",
      "ibm\n",
      "microsoft\n",
      "dream\n",
      "intetest\n",
      "interest\n",
      "me\n",
      "lot\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.add_word(\"好朋友\")\n",
    "for t in analyzer(\"我的好朋友是李明;我爱北京天安门;IBM和Microsoft; I have a dream. this is intetesting and interested me a lot\"):\n",
    "    print(t.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 命令行分词\n",
    "\n",
    "> 使用示例：**python -m jieba news.txt > cut_result.txt**\n",
    "\n",
    "**命令行选项（翻译）：**\n",
    "\n",
    ">使用: python -m jieba [options] filename\n",
    "结巴命令行界面。\n",
    "固定参数:\n",
    "  filename              输入文件\n",
    "可选参数:\n",
    "  -h, --help            显示此帮助信息并退出\n",
    "  -d [DELIM], --delimiter [DELIM]\n",
    "                        使用 DELIM 分隔词语，而不是用默认的' / '。\n",
    "                        若不指定 DELIM，则使用一个空格分隔。\n",
    "  -p [DELIM], --pos [DELIM]\n",
    "                        启用词性标注；如果指定 DELIM，词语和词性之间\n",
    "                        用它分隔，否则用 _ 分隔\n",
    "  -D DICT, --dict DICT  使用 DICT 代替默认词典\n",
    "  -u USER_DICT, --user-dict USER_DICT\n",
    "                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用\n",
    "  -a, --cut-all         全模式分词（不支持词性标注）\n",
    "  -n, --no-hmm          不使用隐含马尔可夫模型\n",
    "  -q, --quiet           不输出载入信息到 STDERR\n",
    "  -V, --version         显示版本信息并退出\n",
    "如果没有指定文件名，则使用标准输入。>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(jieba.analyse.absolute_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pair('试试', 'vn'),\n",
       " pair('好不好', 'l'),\n",
       " pair('吧', 'y'),\n",
       " pair('希望', 'v'),\n",
       " pair('好', 'a'),\n",
       " pair('用', 'p')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.posseg.cut(\"试试好不好吧希望好用\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['试试', '好不好', '吧', '希望', '好用']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut(\"试试好不好吧希望好用\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.set_dictionary(\"dict.txt.small.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAR_STATE_TAB_P',\n",
       " 'POSTokenizer',\n",
       " 'PROB_EMIT_P',\n",
       " 'PROB_START_P',\n",
       " 'PROB_TRANS_P',\n",
       " 'PY2',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_lcut_internal',\n",
       " '_lcut_internal_no_hmm',\n",
       " 'absolute_import',\n",
       " 'char_state_tab',\n",
       " 'char_state_tab_P',\n",
       " 'cut',\n",
       " 'default_encoding',\n",
       " 'dt',\n",
       " 'emit_P',\n",
       " 'get_module_res',\n",
       " 'initialize',\n",
       " 'iteritems',\n",
       " 'iterkeys',\n",
       " 'itervalues',\n",
       " 'jieba',\n",
       " 'lcut',\n",
       " 'load_model',\n",
       " 'os',\n",
       " 'pair',\n",
       " 'pickle',\n",
       " 'pkg_resources',\n",
       " 'prob_emit',\n",
       " 'prob_start',\n",
       " 'prob_trans',\n",
       " 're',\n",
       " 're_eng',\n",
       " 're_eng1',\n",
       " 're_han_detail',\n",
       " 're_han_internal',\n",
       " 're_num',\n",
       " 're_skip_detail',\n",
       " 're_skip_internal',\n",
       " 'resolve_filename',\n",
       " 'start_P',\n",
       " 'strdecode',\n",
       " 'string_types',\n",
       " 'sys',\n",
       " 'text_type',\n",
       " 'trans_P',\n",
       " 'unicode_literals',\n",
       " 'viterbi',\n",
       " 'xrange']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(jieba.posseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
